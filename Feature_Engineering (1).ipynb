{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ques-1 What is a parameter?"
      ],
      "metadata": {
        "id": "ZojqUfmFujwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A parameter is a variable or configuration value used to define or control the behavior of a process that transforms raw data into features suitable for modeling. Parameters guide how the transformation is applied and influence the resulting features.\n",
        "\n",
        "--> Importance of Parameters in Feature Engineering:\n",
        "\n",
        "Model Performance: The choice of parameters can significantly influence the quality of the features and, ultimately, the model's predictive performance.\n",
        "\n",
        "Interpretability: Proper parameter selection ensures meaningful and interpretable features.\n",
        "\n",
        "Efficiency: Efficient parameter choices reduce computation time and avoid overfitting or underfitting.\n",
        "\n",
        "--> Examples of Parameters in Feature Engineering:\n",
        "\n",
        "Scaling and Normalization\n",
        "\n",
        "Binning (Discretization)\n",
        "\n",
        "Encoding Categorical Variables\n",
        "\n",
        "Handling Missing Values\n",
        "\n",
        "Polynomial Features\n",
        "\n",
        "Feature Selection\n"
      ],
      "metadata": {
        "id": "tQ66VTImur4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-2 What is correlation?\n",
        "What does negative correlation mean?"
      ],
      "metadata": {
        "id": "1B-NDWCK0OQC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is used to identify and understand relationships between features (independent variables) and between features and the target variable (dependent variable). This understanding is essential for selecting, transforming, or creating features that improve the performance and interpretability of machine learning models.\n",
        "\n",
        "When two features have a negative correlation, it means that as one feature's value increases, the other feature's value tends to decrease, and vice versa.\n",
        "\n",
        " For example:\n",
        "\n",
        "In a dataset with features like temperature and heating cost, they may have a negative correlation because higher temperatures generally reduce heating costs.\n",
        "\n"
      ],
      "metadata": {
        "id": "UyC41RD40TxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-3 Define Machine Learning. What are the main components in Machine Learning?"
      ],
      "metadata": {
        "id": "PZdzVweK0xzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning (ML) is a subset of artificial intelligence (AI) that focuses on developing algorithms and statistical models that enable computers to learn patterns and make decisions or predictions from data without being explicitly programmed. The key idea is to let machines improve their performance on tasks through experience.\n",
        "\n",
        "--> Main Components in Machine Learning:\n",
        "\n",
        "1. Data- Input to the learning process; raw data and features derived from it.\n",
        "\n",
        "2. Features- Key variables representing the data; foundation for training models.\n",
        "\n",
        "3. Model- Mathematical representation of patterns in data.\n",
        "\n",
        "4. Algorithm- Method for training the model by optimizing its parameters.\n",
        "\n",
        "5. Training- Process of fitting the model to the data.\n",
        "\n",
        "6. Evaluation- Metrics to test how well the model performs.\n",
        "\n",
        "7. Hyperparameters- Settings that control the learning process.\n",
        "\n",
        "8. Deployment- Real-world use of the trained model for decision-making or predictions.\n"
      ],
      "metadata": {
        "id": "mIg6ixpD04RX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-4 How does loss value help in determining whether the model is good or not?"
      ],
      "metadata": {
        "id": "N_8U4wXX1pGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss value is a quantitative measure of how well a machine learning model's predictions align with the actual target values. It plays a crucial role in assessing whether the model is good or not by providing direct feedback on the model's performance during training and evaluation.\n",
        "\n",
        "--> Loss Value Determines Model Quality:\n",
        "\n",
        "1. Measures Prediction Error:\n",
        "\n",
        "The loss value represents the difference (error) between the predicted output and the true output.\n",
        "\n",
        "A low loss value indicates that the model's predictions are close to the actual values, which is desirable.\n",
        "\n",
        "A high loss value indicates that the model is making significant errors, which suggests issues with the model, data, or features.\n",
        "\n",
        "2. Tracks Model Training Progress:\n",
        "\n",
        "During training, the loss value is used to monitor whether the model is improving:\n",
        "\n",
        "Decreasing Loss: Suggests the model is learning and capturing patterns in the data.\n",
        "\n",
        "Stagnant or Increasing Loss: Indicates potential problems such as poor learning rate, insufficient data, or over/underfitting.\n",
        "\n",
        "3. Validation Loss for Generalization:\n",
        "\n",
        "Comparing the loss on the training data to the validation data helps determine whether the model generalizes well:\n",
        "\n",
        "Similar Loss on Training and Validation Data: Indicates good generalization.\n",
        "\n",
        "Low Training Loss, High Validation Loss: Suggests overfitting (model memorizes training data but fails on unseen data).\n",
        "\n",
        "High Loss on Both Training and Validation: Indicates underfitting (model is too simple or lacks sufficient features).\n",
        "\n",
        "4. Basis for Optimization:\n",
        "\n",
        "The loss function guides the model's optimization process by providing a signal to adjust the model's parameters (weights) using techniques like gradient descent.\n",
        "\n",
        "A good model will consistently reduce the loss during training while avoiding overfitting.\n",
        "\n",
        "5. Selecting the Best Model:\n",
        "\n",
        "When comparing multiple models, the one with the lowest validation loss is typically considered the best, assuming it generalizes well."
      ],
      "metadata": {
        "id": "G4gk60sk3U5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques-5 What are continuous and categorical variables?"
      ],
      "metadata": {
        "id": "KtNxSv7X4LCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------>Continuous Variables:\n",
        "Variables that can take any numeric value within a range and have a meaningful order and scale.\n",
        "\n",
        "--> Characteristics:\n",
        "\n",
        "Values are measured on a continuous scale (e.g., decimals or fractions are possible).\n",
        "\n",
        "Typically used for quantities or measurements.\n",
        "\n",
        "The difference and ratio between values are meaningful.\n",
        "\n",
        "--> Examples:\n",
        "Height, Age, Weight, Salary.\n",
        "\n",
        "--> Usage in Machine Learning:\n",
        "\n",
        "Used in regression models or transformed into categories (discretization) for classification tasks.\n",
        "\n",
        "Preprocessing may involve scaling or normalization.\n",
        "\n",
        "------>Categorical Variables:\n",
        "Variables that represent distinct categories or groups.\n",
        "\n",
        "--> Characteristics:\n",
        "\n",
        "Values are discrete and finite (belong to a predefined set of categories).\n",
        "\n",
        "No inherent numeric meaning or order (though some categories may have an ordinal nature).\n",
        "\n",
        "--> Types of Categorical Variables:\n",
        "\n",
        "Nominal:\n",
        "Categories have no specific order.\n",
        "\n",
        "Ordinal:\n",
        "Categories have a meaningful order but no consistent difference between them.\n",
        "\n",
        "--> Example:\n",
        "Gender, Marital Status, Education.\n",
        "\n",
        "--> Usage in Machine Learning:\n",
        "\n",
        "Often encoded using techniques like:\n",
        "\n",
        "One-Hot Encoding: Creates binary columns for each category.\n",
        "\n",
        "Label Encoding: Assigns numeric labels to categories.\n",
        "\n",
        "Target Encoding: Maps categories to statistics (e.g., mean target value for each category).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8iAkaARp4Pe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-6 How do we handle categorical variables in Machine Learning? What are the common techniques?**"
      ],
      "metadata": {
        "id": "c1FSKrbk6fSD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical variables need to be transformed into a numeric format that machine learning algorithms can understand. The choice of technique depends on the nature of the categorical data, the model being used, and the dataset's size and characteristics.\n",
        "\n",
        "Common Techniques for Handling Categorical Variables:\n",
        "\n",
        "1. Label Encoding\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "Example:\n",
        "Color: Red → 0, Blue → 1, Green → 2.\n",
        "\n",
        "Pros:\n",
        "Simple and efficient for ordinal data.\n",
        "\n",
        "Cons:\n",
        "May introduce unintended ordinal relationships for nominal data.\n",
        "\n",
        "Best for: Ordinal variables or tree-based models (e.g., Decision Trees, Random Forests).\n",
        "\n",
        "2. One-Hot Encoding\n",
        "Creates binary columns for each category. A value of 1 indicates the presence of a category, and 0 indicates its absence.\n",
        "\n",
        "Example:\n",
        "Color: Red → [1, 0, 0], Blue → [0, 1, 0], Green → [0, 0, 1].\n",
        "\n",
        "Pros:\n",
        "Avoids introducing ordinal relationships.\n",
        "\n",
        "Works well with nominal data.\n",
        "\n",
        "Cons:\n",
        "Increases dimensionality (curse of dimensionality), especially for high-cardinality features.\n",
        "\n",
        "Best for: Nominal variables, especially for algorithms like Logistic Regression and Neural Networks.\n",
        "\n",
        "3. Ordinal Encoding\n",
        "Encodes categories with integers, respecting their order.\n",
        "\n",
        "Example:\n",
        "Education Level: High School → 0, Bachelor's → 1, Master's → 2, PhD → 3.\n",
        "\n",
        "Pros:\n",
        "Retains the order of categories.\n",
        "\n",
        "Cons:\n",
        "Assumes linear relationships between categories, which may not be appropriate.\n",
        "\n",
        "Best for: Ordinal variables in linear models or decision trees.\n",
        "\n",
        "4. Target Encoding (Mean Encoding)\n",
        "Replaces categories with the mean of the target variable for each category.\n",
        "\n",
        "Example (for a binary target variable):\n",
        "City: Delhi → 0.7, Mumbai → 0.3 (mean target values for each city).\n",
        "\n",
        "Pros:\n",
        "Captures the relationship between the category and the target variable.\n",
        "\n",
        "Cons:\n",
        "Risk of data leakage if not applied carefully (should only be done on training data).\n",
        "\n",
        "Best for: High-cardinality variables in supervised learning tasks.\n",
        "\n",
        "5. Frequency Encoding\n",
        "Encodes categories based on their frequency in the dataset.\n",
        "\n",
        "Example:\n",
        "City: Delhi (50) → 50, Mumbai (30) → 30, Kolkata (20) → 20.\n",
        "\n",
        "Pros:\n",
        "Simple and retains category information.\n",
        "\n",
        "Cons:\n",
        "May not capture the relationship with the target variable.\n",
        "\n",
        "Best for: High-cardinality variables where frequency carries meaningful information.\n",
        "\n",
        "6. Binary Encoding\n",
        "Combines the benefits of One-Hot and Label Encoding by converting categories into binary format.\n",
        "\n",
        "Example:\n",
        "City: Delhi → 1, Mumbai → 2, Kolkata → 3.\n",
        "\n",
        "Binary encoding: Delhi → [0, 1], Mumbai → [1, 0], Kolkata → [1, 1].\n",
        "\n",
        "Pros:\n",
        "Reduces dimensionality compared to One-Hot Encoding.\n",
        "\n",
        "Cons:\n",
        "Interpretation of the binary representation can be complex.\n",
        "\n",
        "Best for: High-cardinality variables in large datasets.\n",
        "\n",
        "7. Hash Encoding\n",
        "Uses a hash function to map categories to a fixed number of columns.\n",
        "\n",
        "Example:\n",
        "Hash categories into n buckets (user-defined size).\n",
        "\n",
        "Pros:\n",
        "Handles high-cardinality features efficiently.\n",
        "\n",
        "Avoids large memory usage.\n",
        "\n",
        "Cons:\n",
        "Risk of hash collisions (two categories mapped to the same bucket).\n",
        "\n",
        "Best for: Large datasets with high-cardinality features.\n",
        "\n",
        "8. Custom Encoding (Domain-Specific)\n",
        "Encode categories based on domain knowledge.\n",
        "\n",
        "Example:\n",
        "In a medical dataset, encode Risk Level (Low, Medium, High) based on expert-defined weights (e.g., Low → 1, Medium → 5, High → 10).\n",
        "\n",
        "Pros:\n",
        "Leverages domain knowledge for better feature representation.\n",
        "\n",
        "Cons:\n",
        "Time-consuming and subjective.\n",
        "\n",
        "Best for: When domain expertise is available.\n"
      ],
      "metadata": {
        "id": "i8eWMWkT6p8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-7 What do you mean by training and testing a dataset?**"
      ],
      "metadata": {
        "id": "mg-m79Qb9Ofa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, training and testing datasets are subsets of the original dataset used to build and evaluate a model. Properly splitting and utilizing these datasets ensures that the model can learn from data and generalize well to unseen data.\n",
        "\n",
        "1. Training Dataset\n",
        "\n",
        "Definition:\n",
        "The portion of the dataset used to train (fit) the machine learning model by optimizing its parameters.\n",
        "\n",
        "Purpose:\n",
        "To enable the model to learn patterns, relationships, and rules from the data.\n",
        "\n",
        "Usage:\n",
        "The model \"sees\" and \"learns\" from this data repeatedly during the training phase.\n",
        "\n",
        "Characteristics:\n",
        "Large enough to provide sufficient information for the model to learn effectively.\n",
        "\n",
        "The training process involves minimizing a loss function using optimization algorithms (e.g., Gradient Descent).\n",
        "\n",
        "2. Testing Dataset\n",
        "\n",
        "Definition:\n",
        "The portion of the dataset used to evaluate the model's performance after it has been trained.\n",
        "\n",
        "Purpose:\n",
        "To test the model's ability to generalize its learning to unseen data.\n",
        "\n",
        "Usage:\n",
        "The testing dataset is kept separate from the training process and is only used for evaluation.\n",
        "\n",
        "Characteristics:\n",
        "Represents unseen or new data that the model has not encountered during training.\n",
        "\n",
        "Provides an unbiased assessment of the model's performance."
      ],
      "metadata": {
        "id": "uO4VJpZH9nXD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-8 What is sklearn.preprocessing?**"
      ],
      "metadata": {
        "id": "qlJjGpgQ_DKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sklearn.preprocessing module in Scikit-Learn provides tools to transform raw data into a format suitable for machine learning algorithms. Many algorithms perform better when the data is properly scaled, normalized, or encoded, making preprocessing a crucial step in any machine learning pipeline.\n",
        "\n",
        "Key Techniques:\n",
        "\n",
        "1. Scaling and Standardization:\n",
        "\n",
        "StandardScaler: Scales features to a mean of 0 and standard deviation of 1.\n",
        "\n",
        "MinMaxScaler: Rescales data to a specific range, often [0, 1].\n",
        "\n",
        "RobustScaler: Uses median and interquartile range, robust to outliers.\n",
        "\n",
        "2. Encoding Categorical Variables:\n",
        "\n",
        "LabelEncoder: Converts categories into integers.\n",
        "\n",
        "OneHotEncoder: Creates binary columns for each category.\n",
        "\n",
        "OrdinalEncoder: Encodes categories while preserving order.\n",
        "\n",
        "3. Polynomial Features:\n",
        "\n",
        "Generates polynomial and interaction terms to capture non-linear relationships.\n",
        "\n",
        "4. Normalization:\n",
        "\n",
        "Scales feature vectors to unit norm using Normalizer.\n",
        "\n",
        "5. Handling Missing Data:\n",
        "\n",
        "SimpleImputer fills missing values with the mean, median, or constant.\n",
        "\n",
        "KNNImputer imputes based on nearest neighbors.\n",
        "\n",
        "6. Binarization and Discretization:\n",
        "\n",
        "Binarizer thresholds values; KBinsDiscretizer bins continuous data."
      ],
      "metadata": {
        "id": "E57-_on7_yKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-9 What is a Test set?**"
      ],
      "metadata": {
        "id": "PV2aq479BDad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test set is a portion of the dataset used to evaluate the performance of a trained machine learning model. It is separated from the training process to ensure an unbiased assessment of the model's ability to generalize to unseen data.\n",
        "\n",
        "Key Characteristics of a Test Set\n",
        "\n",
        "1. Unseen by the Model:\n",
        "\n",
        "The test set is not used during training or validation, ensuring the model has no prior exposure to this data.\n",
        "\n",
        "It helps evaluate the model's ability to generalize to new, real-world data.\n",
        "\n",
        "2. Fixed Partition:\n",
        "\n",
        "Typically, the dataset is divided into training, validation (optional), and test sets.\n",
        "\n",
        "Common splits: 70-80% for training, 20-30% for testing.\n",
        "\n",
        "3. Independent Evaluation:\n",
        "\n",
        "Metrics like accuracy, precision, recall, F1-score, and RMSE are computed on the test set to determine the model’s final performance."
      ],
      "metadata": {
        "id": "VZXA_iyLBMx8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-10 How do we split data for model fitting (training and testing) in Python?\n",
        "How do you approach a Machine Learning problem?**"
      ],
      "metadata": {
        "id": "lJSFuzFrBlAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Splitting Data for Model Fitting in Python\n",
        "To split data into training and testing sets in Python, you can use Scikit-Learn's train_test_split function. This function randomly partitions the dataset into specified proportions for training and testing. Here's how it's done:"
      ],
      "metadata": {
        "id": "DC9VSOigBtEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALP5xEAPt5jM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = pd.DataFrame({\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [10, 20, 30, 40, 50],\n",
        "    'target': [0, 1, 0, 1, 0]\n",
        "})\n",
        "\n",
        "# Features (X) and target (y)\n",
        "X = data[['feature1', 'feature2']]\n",
        "y = data['target']\n",
        "\n",
        "# Split data: 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Check split sizes\n",
        "print(\"Training features:\\n\", X_train)\n",
        "print(\"Testing features:\\n\", X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test_size: Proportion of the dataset to include in the test split (e.g., 0.2 for 20%).\n",
        "\n",
        "random_state: Ensures reproducibility of the split by setting a random seed.\n",
        "\n",
        "stratify: Ensures class distribution in the train and test sets matches the original dataset (useful for classification problems)."
      ],
      "metadata": {
        "id": "GoT-nlwjCMqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How to Approach a Machine Learning Problem\n",
        "A structured approach helps efficiently solve machine learning problems. Here's a common step-by-step workflow:\n",
        "\n",
        "Step 1: Define the Problem\n",
        "\n",
        "Understand Objectives: Identify what you are solving (e.g., classification, regression).\n",
        "\n",
        "Understand the Data: Study the dataset's features and target variable.\n",
        "\n",
        "Step 2: Collect and Explore Data\n",
        "\n",
        "Data Collection: Gather data from sources like databases, APIs, or files.\n",
        "\n",
        "Exploratory Data Analysis (EDA): Analyze data using summary statistics, visualizations, and identifying patterns or trends.\n",
        "\n",
        "Step 3: Preprocess the Data\n",
        "\n",
        "Handle Missing Values: Use techniques like imputation or removing rows/columns.\n",
        "\n",
        "Encode Categorical Variables: Use LabelEncoder or OneHotEncoder.\n",
        "\n",
        "Feature Scaling: Apply StandardScaler or MinMaxScaler for normalization.\n",
        "\n",
        "Outlier Treatment: Identify and handle outliers using domain knowledge or statistical methods.\n",
        "\n",
        "Step 4: Split the Dataset\n",
        "\n",
        "Use train_test_split to divide data into training and testing sets.\n",
        "Optionally create a validation set for hyperparameter tuning.\n",
        "\n",
        "Step 5: Select and Train a Model\n",
        "\n",
        "Choose a Model: Select an appropriate algorithm (e.g., Decision Trees, Linear Regression).\n",
        "\n",
        "Train the Model: Fit the model using the training dataset.\n",
        "\n",
        "Step 6: Evaluate the Model\n",
        "\n",
        "Test Performance: Use metrics like accuracy, precision, recall, or RMSE on the test set.\n",
        "\n",
        "Cross-Validation: Use KFold or StratifiedKFold for robust performance evaluation.\n",
        "\n",
        "Step 7: Optimize the Model\n",
        "\n",
        "Hyperparameter Tuning: Use techniques like Grid Search or Random Search (GridSearchCV or RandomizedSearchCV).\n",
        "\n",
        "Feature Engineering: Create new features or remove irrelevant ones.\n",
        "\n",
        "Step 8: Deploy and Monitor\n",
        "\n",
        "Deploy the model in production (e.g., APIs or dashboards).\n",
        "\n",
        "Continuously monitor the model's performance and update it as necessary."
      ],
      "metadata": {
        "id": "Ob4LtKMDCUMR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-11 Why do we have to perform EDA before fitting a model to the data?**"
      ],
      "metadata": {
        "id": "Ng5Qpj9QC91q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is crucial for understanding the dataset, identifying potential issues, and guiding the modeling process. EDA helps ensure that the model is applied to clean, well-understood data, which increases the chances of successful and accurate predictions. Here's why EDA is essential:\n",
        "\n",
        "1. Understand the Data Distribution\n",
        "\n",
        "Purpose: To understand the range, central tendency (mean, median), and spread (variance) of the features and target variable.\n",
        "\n",
        "Benefit: This helps identify whether features need scaling (e.g., for models like Logistic Regression, KNN) and if any data transformations (e.g., logarithmic or polynomial transformations) might be beneficial.\n",
        "\n",
        "2. Detect Missing or Inconsistent Data\n",
        "\n",
        "Purpose: Identify missing values, null entries, or outliers.\n",
        "\n",
        "Benefit: Missing values or inconsistent data can significantly affect model performance. EDA helps you decide how to handle them (e.g., imputation, removal, or interpolation).\n",
        "\n",
        "3. Identify Relationships and Correlations\n",
        "\n",
        "Purpose: Understand relationships between features and the target variable, as well as correlations between features.\n",
        "\n",
        "Benefit: Correlation analysis helps identify redundant features that might be dropped (to reduce multicollinearity) or features that should be engineered further. It also allows the identification of important predictor variables that strongly influence the target.\n",
        "\n",
        "4. Check for Outliers and Anomalies\n",
        "\n",
        "Purpose: Visualize and identify extreme values (outliers).\n",
        "\n",
        "Benefit: Outliers can distort model performance, especially in algorithms sensitive to data scale (e.g., linear models or KNN). EDA helps detect these outliers so you can handle them appropriately (e.g., through removal, transformation, or capping).\n",
        "\n",
        "5. Detect Data Distribution Issues (Skewness and Kurtosis)\n",
        "\n",
        "Purpose: Check whether features follow a normal distribution or if they are skewed.\n",
        "\n",
        "Benefit: If data is highly skewed, some models may struggle. Techniques like log transformations or using non-parametric models can be applied to address such issues.\n",
        "\n",
        "6. Visualize Class Imbalance (for Classification Problems)\n",
        "\n",
        "Purpose: Analyze the distribution of different classes in the target variable.\n",
        "\n",
        "Benefit: If the target variable has imbalanced classes, this can lead to biased predictions. Techniques such as oversampling, undersampling, or using specialized algorithms like SMOTE can be applied to address this issue.\n",
        "\n",
        "7. Identify Feature Relationships (Multicollinearity)\n",
        "\n",
        "Purpose: Understand how features interact with each other.\n",
        "\n",
        "Benefit: Highly correlated features (multicollinearity) can cause instability in some models (e.g., in linear regression). EDA helps in identifying such correlations, leading to potential feature reduction or transformation.\n",
        "\n",
        "8. Feature Engineering\n",
        "\n",
        "Purpose: Discover patterns and trends to create new features or transform existing ones.\n",
        "\n",
        "Benefit: EDA helps in generating new features, such as combining multiple features or creating interaction terms, which can enhance model performance.\n",
        "\n",
        "9. Validate Assumptions for Model Selection\n",
        "\n",
        "Purpose: Check whether the assumptions of your chosen model hold true.\n",
        "\n",
        "Benefit: For example, linear regression assumes a linear relationship between features and the target. EDA helps in verifying such assumptions before applying the model.\n"
      ],
      "metadata": {
        "id": "EGaqkgU2DGCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-12 What is correlation?**"
      ],
      "metadata": {
        "id": "N2GI8vmsOyU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation refers to a statistical measure that expresses the extent to which two variables are related to each other. In other words, it quantifies how changes in one variable are associated with changes in another. The value of correlation ranges from -1 to +1:\n",
        "\n",
        "+1: Perfect positive correlation. As one variable increases, the other increases in a perfectly linear relationship.\n",
        "\n",
        "0: No correlation. There is no linear relationship between the two variables.\n",
        "\n",
        "-1: Perfect negative correlation. As one variable increases, the other decreases in a perfectly linear relationship."
      ],
      "metadata": {
        "id": "SD9-rw8VUhEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-13 What does negative correlation mean?**"
      ],
      "metadata": {
        "id": "Joj84RrVUpth"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. In other words, there is an inverse relationship between the two variables.\n",
        "\n",
        "-1: Perfect negative correlation — a perfect inverse relationship, where the variables move in opposite directions in a perfectly linear way.\n",
        "\n",
        "Example of Negative Correlation\n",
        "\n",
        "Temperature and Heating Costs:\n",
        "\n",
        "As the outside temperature increases, the need for heating decreases. In this case, there is a negative correlation between temperature and heating costs."
      ],
      "metadata": {
        "id": "7C_Nt338Uuja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-14 How can you find correlation between variables in Python?**"
      ],
      "metadata": {
        "id": "uzXMOf2hVAeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Using Pandas .corr() Method\n",
        "The .corr() method in Pandas computes the Pearson correlation coefficient between numerical columns in a DataFrame by default. It returns a correlation matrix, where each value represents the correlation between two variables."
      ],
      "metadata": {
        "id": "iZPkMCh2VGL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'feature1': [1, 2, 3, 4, 5],\n",
        "    'feature2': [10, 20, 30, 40, 50],\n",
        "    'feature3': [5, 4, 3, 2, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeMxPtaAC7-t",
        "outputId": "7aa42f22-75c5-4801-8ff1-509edf5057d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          feature1  feature2  feature3\n",
            "feature1       1.0       1.0      -1.0\n",
            "feature2       1.0       1.0      -1.0\n",
            "feature3      -1.0      -1.0       1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.0 indicates a perfect positive correlation.\n",
        "\n",
        "-1.0 indicates a perfect negative correlation.\n",
        "\n",
        "0.0 indicates no correlation."
      ],
      "metadata": {
        "id": "JfMGQzklVs5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Visualizing the Correlation Matrix with a Heatmap\n",
        "To get a more intuitive understanding of the correlations, you can visualize the correlation matrix using a heatmap with Seaborn."
      ],
      "metadata": {
        "id": "agBAm60XVz6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a heatmap of the correlation matrix\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "yQXIwLcWViOY",
        "outputId": "87282768-800e-4d84-b82e-671c05428c8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Red indicates strong positive correlation.\n",
        "\n",
        "Blue indicates strong negative correlation.\n",
        "\n",
        "White indicates no correlation."
      ],
      "metadata": {
        "id": "64QpyaWJV9LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Calculating Specific Pairwise Correlation\n",
        "If you're interested in the correlation between just two specific variables, you can do so like this:\n"
      ],
      "metadata": {
        "id": "m-XLY3csWMjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation between feature1 and feature2\n",
        "correlation_value = df['feature1'].corr(df['feature2'])\n",
        "print(\"Correlation between feature1 and feature2:\", correlation_value)\n"
      ],
      "metadata": {
        "id": "k0iUCMkUV7gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-15 What is causation? Explain difference between correlation and causation with an example.**"
      ],
      "metadata": {
        "id": "zaVyAO5UWUSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Causation refers to a direct cause-and-effect relationship where one variable (the cause) directly influences another variable (the effect). This relationship implies that changes in the independent variable directly lead to changes in the dependent variable. Causation is much stronger than correlation because it establishes that one factor is responsible for bringing about an outcome.\n",
        "\n",
        "For example, smoking causes lung cancer. In this case, smoking is the cause, and lung cancer is the effect. The direct biological mechanisms involved in smoking, such as the introduction of harmful chemicals into the lungs, lead to the development of cancer. Therefore, causation explains why one variable changes due to the other.\n",
        "\n",
        "--> Difference\n",
        "\n",
        "The primary difference between correlation and causation is that correlation only shows a relationship, while causation indicates a direct influence of one variable on another. Correlation does not imply that one variable causes the other to change; it only suggests that they are related in some way. Causation, on the other hand, provides a deeper understanding of how and why changes in one variable lead to changes in another.\n",
        "\n",
        "For example, while ice cream sales and temperature are correlated, it is not accurate to say that rising temperatures cause people to buy more ice cream. The relationship could be due to other confounding factors like seasonality. In contrast, smoking leading to lung cancer is a clear example of causation because there is a well-established mechanism that links smoking directly to the disease."
      ],
      "metadata": {
        "id": "jpJ6oPy3We-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-16 What is an Optimizer? What are different types of optimizers? Explain each with an example.**"
      ],
      "metadata": {
        "id": "9gMOYqXkXC4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An optimizer is an algorithm or method used to adjust the weights of a model during the training process in order to minimize the loss function (i.e., the error between the predicted output and the actual output). The goal of optimization is to find the optimal parameters (weights and biases) that minimize the loss, thereby improving the model's accuracy.\n",
        "\n",
        "In simple terms, optimizers adjust the parameters of the model based on the gradients of the loss function with respect to those parameters. The optimizer updates the model’s weights in a way that helps reduce the error and make predictions more accurate.\n",
        "\n",
        "--> Different Types of Optimizers\n",
        "\n",
        "1. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Explanation: SGD is a simple yet widely used optimization algorithm. It updates the model's parameters by calculating the gradient of the loss function with respect to the model parameters, and then adjusts the parameters in the direction that minimizes the loss. Instead of computing the gradient using the entire dataset (which can be computationally expensive), SGD computes the gradient using one data point at a time, which makes it much faster for large datasets.\n",
        "\n",
        "Example:\n",
        "Suppose you want to optimize a simple linear regression model. At each iteration, SGD would select a random data point, calculate the gradient of the loss (difference between predicted and actual values), and update the model’s weights accordingly.\n",
        "\n",
        "Pros:\n",
        "Computationally efficient for large datasets.\n",
        "\n",
        "Can escape local minima.\n",
        "\n",
        "Cons:\n",
        "Noisy updates, leading to less precise convergence.\n",
        "\n",
        "Requires careful tuning of the learning rate.\n",
        "\n",
        "2. Mini-Batch Gradient Descent\n",
        "\n",
        "Explanation: Mini-batch gradient descent is a variation of SGD where instead of using a single data point, it uses a small batch of data to compute the gradient and update the model parameters. This approach strikes a balance between the computational efficiency of full-batch gradient descent and the faster convergence of SGD.\n",
        "\n",
        "Example:\n",
        "If your dataset has 1000 samples, you might set a batch size of 50. Instead of calculating the gradient using the entire dataset or a single point, it would calculate the gradient using 50 data points at each step.\n",
        "\n",
        "Pros:\n",
        "Faster convergence than SGD.\n",
        "\n",
        "Can take advantage of vectorization in matrix operations.\n",
        "\n",
        "Cons:\n",
        "Requires more memory to store the mini-batches.\n",
        "\n",
        "Still might need careful learning rate tuning.\n",
        "\n",
        "3. Momentum\n",
        "\n",
        "Explanation: Momentum is an extension of gradient descent that helps accelerate the convergence by adding a fraction of the previous update to the current one. This helps smooth out the updates and prevents oscillations, allowing the optimizer to converge faster.\n",
        "\n",
        "Example:\n",
        "In gradient descent, the model might oscillate back and forth across a steep part of the loss surface. With momentum, the optimizer builds up velocity in a particular direction, helping it overcome this oscillation.\n",
        "\n",
        "Pros:\n",
        "Reduces oscillations and speeds up convergence.\n",
        "\n",
        "Cons:\n",
        "Requires tuning of the momentum parameter (usually denoted as β).\n",
        "\n",
        "4. RMSprop (Root Mean Square Propagation)\n",
        "\n",
        "Explanation: RMSprop is an adaptive learning rate optimizer that adjusts the learning rate based on the average of recent gradients. It divides the learning rate by a moving average of recent gradient magnitudes, which helps in dealing with the issue of vanishing and exploding gradients in deep networks.\n",
        "\n",
        "Example:\n",
        "In RMSprop, if a particular parameter has large gradients in previous iterations, the learning rate will decrease for that parameter. If the gradient is small, the learning rate increases, ensuring stable convergence.\n",
        "\n",
        "Pros:\n",
        "Works well for non-stationary objectives (e.g., in RNNs).\n",
        "\n",
        "Handles sparse data better than SGD.\n",
        "\n",
        "Cons:\n",
        "Still requires tuning of the learning rate and decay factor.\n",
        "\n",
        "5. Adam (Adaptive Moment Estimation)\n",
        "\n",
        "Explanation: Adam is an extension of momentum and RMSprop that combines both the advantages of momentum (keeping track of past gradients) and RMSprop (adjusting the learning rate based on recent gradient magnitudes). Adam computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients (mean and variance).\n",
        "\n",
        "Example:\n",
        "During training, Adam adjusts the learning rates for each parameter individually based on the gradients, making it highly efficient for large datasets and deep networks. It uses two key parameters: the learning rate (α), and two decay rates for the first and second moment estimates (β1 and β2).\n",
        "\n",
        "Pros:\n",
        "Works well for both sparse and dense gradients.\n",
        "\n",
        "Typically faster convergence and less parameter tuning required compared to SGD.\n",
        "\n",
        "Cons:\n",
        "Can be computationally more expensive than SGD.\n",
        "\n",
        "May sometimes lead to suboptimal performance on certain tasks if not properly tuned.\n",
        "\n",
        "6. Adagrad (Adaptive Gradient Algorithm)\n",
        "\n",
        "Explanation: Adagrad is an adaptive optimizer that adjusts the learning rate for each parameter based on its past gradients. It assigns a larger learning rate to parameters that have smaller gradients and a smaller learning rate to parameters that have larger gradients, aiming to speed up the learning process for less frequently updated parameters.\n",
        "\n",
        "Example:\n",
        "For parameters that are frequently updated (i.e., those with large gradients), Adagrad reduces the learning rate over time. For parameters with infrequent updates, it increases the learning rate.\n",
        "\n",
        "Pros:\n",
        "Ideal for sparse data or tasks where some features appear infrequently.\n",
        "\n",
        "Cons:\n",
        "The learning rate may decrease too rapidly, leading to premature convergence.\n",
        "\n",
        "7. Adadelta\n",
        "\n",
        "Explanation: Adadelta is an improvement on Adagrad, addressing the issue of rapidly decreasing learning rates by using a moving window of past gradients, rather than accumulating all previous gradients. This allows for more stable and efficient updates.\n",
        "\n",
        "Example:\n",
        "Adadelta adjusts the learning rate by considering only a fixed window of past gradients, ensuring that the learning rate doesn’t decrease too quickly, and updates continue efficiently.\n",
        "\n",
        "Pros:\n",
        "More stable than Adagrad in terms of learning rate decay.\n",
        "\n",
        "Cons:\n",
        "More computationally complex than simple SGD.\n"
      ],
      "metadata": {
        "id": "ZvtqrhSpXJ4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-17 What is sklearn.linear_model ?**"
      ],
      "metadata": {
        "id": "v7v0RsMjaf63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sklearn.linear_model is a module in the Scikit-learn library that provides a collection of machine learning models and tools specifically designed for linear methods. Linear models are algorithms that predict an output based on a linear combination of input features. This module contains implementations of various linear regression, classification, and regularization techniques.\n",
        "\n",
        "\n",
        "--> Key Features of sklearn.linear_model:\n",
        "\n",
        "Regression Models:\n",
        "Used for predicting continuous target variables.\n",
        "\n",
        "Examples: Simple Linear Regression, Ridge Regression, Lasso Regression.\n",
        "\n",
        "Classification Models:\n",
        "Used for predicting categorical target variables.\n",
        "\n",
        "Examples: Logistic Regression, Perceptron.\n",
        "\n",
        "Regularization Techniques:\n",
        "Helps prevent overfitting by adding penalties to the model's complexity.\n",
        "\n",
        "Examples: Ridge, Lasso, ElasticNet.\n",
        "\n",
        "Scalable and Efficient:\n",
        "Designed to handle both small and large datasets efficiently."
      ],
      "metadata": {
        "id": "6yT-sVeuMN62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-18 What does model.fit() do? What arguments must be given?**"
      ],
      "metadata": {
        "id": "S_k9Tn_2MjCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.fit() method is used to train a machine learning model on a given dataset. It performs the following key steps:\n",
        "\n",
        "Accepts Input Data:\n",
        "Takes the training data (X) and the corresponding target labels (y).\n",
        "\n",
        "Learns Model Parameters:\n",
        "Based on the input data, it adjusts the model's parameters (e.g., weights, biases) to minimize the error or loss function.\n",
        "\n",
        "Prepares the Model for Predictions:\n",
        "After training, the model can use the learned parameters to make predictions on new, unseen data using model.predict().\n",
        "\n",
        "--> Arguments Required for model.fit()\n",
        "\n",
        "The arguments required depend on the type of model being used (e.g., regression, classification). Generally, it requires:\n",
        "\n",
        "X (Features or Input Data):\n",
        "A 2D array-like object, where each row represents a sample, and each column represents a feature.\n",
        "\n",
        "Shape: (n_samples, n_features).\n",
        "\n",
        "y (Target or Labels):\n",
        "A 1D array-like object, where each element is the target value for the corresponding sample in X.\n",
        "\n",
        "Shape: (n_samples,).\n",
        "\n",
        "For example:\n",
        "\n",
        "Regression models: y contains continuous values.\n",
        "\n",
        "Classification models: y contains categorical class labels.\n"
      ],
      "metadata": {
        "id": "SMRQvDaGMpzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-19 What does model.predict() do? What arguments must be given?**"
      ],
      "metadata": {
        "id": "Y_L7zTaMNKVK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model.predict() method is used to make predictions based on a trained machine learning model. Once the model has been trained using model.fit(), you can use model.predict() to predict the output (target values) for new, unseen input data.\n",
        "\n",
        "--> Functionality of model.predict()\n",
        "\n",
        "Takes Input Data (X):\n",
        "It accepts a set of input features for which predictions are required.\n",
        "\n",
        "Uses Trained Parameters:\n",
        "The model applies the learned parameters (e.g., weights and biases) from the training phase to calculate the predicted output.\n",
        "\n",
        "Returns Predicted Values:\n",
        "For regression tasks: It returns continuous numeric values.\n",
        "\n",
        "For classification tasks: It returns predicted class labels.\n",
        "\n",
        "--> Arguments Required for model.predict()\n",
        "\n",
        "X (Input Features):\n",
        "\n",
        "A 2D array-like object where:\n",
        "\n",
        "Each row represents a sample.\n",
        "\n",
        "Each column represents a feature.\n",
        "\n",
        "Shape: (n_samples, n_features).\n",
        "\n",
        "Example: If you trained your model with a dataset that has 3 features per sample, then X should have the same number of features for each input sample."
      ],
      "metadata": {
        "id": "hkNJhlqcNPF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-21 What is feature scaling? How does it help in Machine Learning?**"
      ],
      "metadata": {
        "id": "68xk8DB_OFaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is a preprocessing technique used to normalize the range of independent variables or features of data. In feature scaling, the values of numerical features are transformed to fall within a specific range or have a particular distribution, typically to make them comparable and improve the performance of machine learning algorithms.\n",
        "\n",
        "\n",
        "--> How Feature Scaling Helps in Machine Learning\n",
        "\n",
        "Prevents Bias in Distance Metrics:\n",
        "Scaling ensures that features with larger ranges do not dominate algorithms relying on distance, like KNN, K-Means, or SVM.\n",
        "\n",
        "Faster Optimization:\n",
        "For gradient-based algorithms, scaling avoids large variations in feature magnitudes that slow down convergence.\n",
        "\n",
        "Improves Accuracy:\n",
        "Models trained on scaled features often generalize better as no feature disproportionately influences the outcome.\n",
        "\n",
        "Essential for Regularized Models:\n",
        "Scaling ensures uniformity, making regularization (e.g., L1/L2 penalties) effective across all features.\n"
      ],
      "metadata": {
        "id": "XGTLLgu7OQbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-22 How do we perform scaling in Python?**"
      ],
      "metadata": {
        "id": "9cnuwtB1OtZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import Required Libraries\n",
        "Start by importing the necessary libraries:"
      ],
      "metadata": {
        "id": "Kdw_p2IaO6Nv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n"
      ],
      "metadata": {
        "id": "49ADeDwEWeJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the Dataset\n",
        "Create or load your dataset:"
      ],
      "metadata": {
        "id": "7NQpfJKhPg2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Example dataset\n",
        "data = {'Feature1': [10, 20, 30, 40, 50],\n",
        "        'Feature2': [100, 200, 300, 400, 500],\n",
        "        'Feature3': [1, 2, 3, 4, 5]}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "Lx0oPpDBPiwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Apply Feature Scaling Techniques\n",
        "\n",
        "a) Min-Max Scaling\n",
        "Scales features to a specific range, typically [0, 1]."
      ],
      "metadata": {
        "id": "ZhtrxJGUPkxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "print(pd.DataFrame(scaled_data, columns=df.columns))\n"
      ],
      "metadata": {
        "id": "3hdQdJ40PoT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b) Standardization (Z-score Normalization)\n",
        "Scales features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "tMyJGthNPtcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "print(pd.DataFrame(scaled_data, columns=df.columns))\n"
      ],
      "metadata": {
        "id": "c79LM28qPzb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c) Robust Scaling\n",
        "Scales data using the median and interquartile range, making it robust to outliers.\n"
      ],
      "metadata": {
        "id": "ODURUrwQP1ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "print(pd.DataFrame(scaled_data, columns=df.columns))\n"
      ],
      "metadata": {
        "id": "g10NGASMP3hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d) Max Abs Scaling\n",
        "Scales each feature by dividing by its maximum absolute value. Maintains sparsity for sparse datasets."
      ],
      "metadata": {
        "id": "4ijlR1QLP5Ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MaxAbsScaler()\n",
        "scaled_data = scaler.fit_transform(df)\n",
        "print(pd.DataFrame(scaled_data, columns=df.columns))\n"
      ],
      "metadata": {
        "id": "0AfLcF02P7Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Transform New Data\n",
        "Once the scaler is fitted to the training data, you can use it to scale new or test data:\n",
        "\n"
      ],
      "metadata": {
        "id": "yt4H4PSnP8wF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = [[60, 600, 6]]\n",
        "scaled_new_data = scaler.transform(new_data)\n",
        "print(scaled_new_data)\n"
      ],
      "metadata": {
        "id": "kTL84xtZP_-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Inverse Transform (Optional)\n",
        "To reverse the scaling and return to the original scale:"
      ],
      "metadata": {
        "id": "LX4XVlShQDQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_data = scaler.inverse_transform(scaled_data)\n",
        "print(pd.DataFrame(original_data, columns=df.columns))\n"
      ],
      "metadata": {
        "id": "va_8AMX-QGO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ques-25 Explain data encoding?**"
      ],
      "metadata": {
        "id": "1zhYx4zKQIFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data encoding is the process of converting categorical data into a numerical format that machine learning algorithms can understand. Since most machine learning models work with numerical inputs, encoding is a crucial step in preprocessing categorical variables in datasets.\n",
        "\n",
        "Categorical data can be of two types:\n",
        "\n",
        "Nominal: Categories without any inherent order (e.g., colors: red, green, blue).\n",
        "\n",
        "Ordinal: Categories with a meaningful order (e.g., size: small, medium, large).\n",
        "\n",
        "--> Why is Data Encoding Necessary?\n",
        "\n",
        "Machine Learning Models Require Numerical Data:\n",
        "Algorithms like regression, SVM, and neural networks cannot process categorical values directly.\n",
        "\n",
        "Preserves Relationships:\n",
        "Proper encoding ensures that relationships between categories are maintained in the transformed data.\n",
        "\n",
        "Improves Model Performance:\n",
        "Encoded data helps models learn and generalize better, especially for classification tasks.\n",
        "\n",
        "--> Common Data Encoding Techniques:\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "2. One-Hot Encoding\n",
        "\n",
        "3. Ordinal Encoding\n",
        "\n",
        "4. Frequency Encoding\n",
        "\n",
        "5. Binary Encoding\n",
        "\n",
        "6. Target Encoding\n",
        "\n",
        "--> Which Encoding to Use?\n",
        "\n",
        "Small number of categories: Use One-Hot Encoding.\n",
        "\n",
        "Ordinal categories: Use Ordinal Encoding or Label Encoding.\n",
        "\n",
        "High-cardinality categories: Use Target Encoding or Frequency Encoding.\n",
        "\n",
        "Memory constraints: Use Label or Binary Encoding."
      ],
      "metadata": {
        "id": "lZc7g1ZSQVPS"
      }
    }
  ]
}